{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to R & Jupyter Notebooks\n",
    "# Probablistic & Statistical Models in R\n",
    "\n",
    "Matthew D. Turner, PhD  \n",
    "Georgia State University\n",
    "\n",
    "Some rights reserved: [cc by-nc-sa](https://creativecommons.org/licenses/by-nc-sa/4.0/) See bottom of document for details.\n",
    "***\n",
    "> “As data scientists, our job is to extract signal from noise.”\n",
    "―Daniel Tunkelang, [Source](https://books.google.com/books?id=86jhBQAAQBAJ&pg=PA92&lpg=PA92&dq=As+data+scientists,+our+job+is+to+extract+signal+from+noise.&source=bl&ots=to5evsRfoA&sig=HUz0QlUYhV-6FXCGaFx8ZYRowls&hl=en&sa=X&ved=0ahUKEwjiys_Fpv_SAhXCq1QKHb_sBMsQ6AEIGjAA#v=onepage&q=As%20data%20scientists%2C%20our%20job%20is%20to%20extract%20signal%20from%20noise.&f=false)\n",
    "***\n",
    "In this document we will go through a sequence of building simple probability models for some standard situations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review: Loading Data from Files & Data Frames\n",
    "You should have already done this in a previous notebook.\n",
    "\n",
    "The most common format for data exchange in the world is plain text files in the comma seperated value format. The most common way to identify these **CSV** files is with the file extension \".csv\". Despite its name, this format also includes tab-seperated values as well, although these are sometimes called **TSV** files and labeled with \".tsv\".\n",
    "\n",
    "Your best bet is to always EXPORT (or save) the data from any given program in the CSV format, then IMPORT or load this file into R. There are packages that will read other programs data files, but usually the results are worse than using CSV as the sharing format. And most other programs can export to CSV, although they may hide this feature from the user.\n",
    "\n",
    "To read CSV files into R, we use `read.csv()` like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hw2 <- read.csv(\"height_weight_200.csv\")  # Some data on men"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you read in a CSV, you need to put the result in a variable (here, `hw2`) and this will by default be a data frame. If you try to import from other software weird things can happen. Choose CSV files whenever you can!\n",
    "\n",
    ">**Aside:** If you would like to inspect the original data file, select the main Jupyter tab in your browser and click on `height_weight_200.csv`. Jupyter will show you the contents in another browser tab. Note that this new view of the file is editable and so you can change the data manually if you like. But don't. That will mess some stuff up below!\n",
    "\n",
    "Once the data is loaded into the data frame `hw2` we can look at its basic features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(hw2)   # Summarize each variable (height in inches; weight in pounds)\n",
    "nrow(hw2)      # How many rows in the table? This is the number of subjects (200)\n",
    "head(hw2)      # Print out the first few rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To access one variable in a frame, use the $ (dollar sign):\n",
    "\n",
    "hw2$height   # This just returns all 200 heights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, when using a data frame, you have to include the name of the frame in front of the variable name and put a dollar sign (`$`) between them. In the example above, the data frame name is `hw2` and the variable is `height`. This means that the full name and address of the data is `hw2$height`.\n",
    "\n",
    "In the previous notebooks, you learned more about accessing and indexing data. We will not review all of this here.\n",
    "***\n",
    "## 4.1 Visualizing Simple Distributions: Histograms\n",
    "In modern statistical practice, it is standard practice to start with a visualization of data. The easiset way to visualize univariate (one variable) data is to use a histogram. In R, histograms come from the `hist` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you would like to know more about the hist command, use ? here. \n",
    "#     Or skip it if you just want to see hist in action\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `hist` command takes many possible options, although the defaults work well for quick graphs. You should be able to figure out the options used in the example below this just by reading and thinking. We continus to use the `options` settings to make the figures fit into the Jupyter notebook nicely. The defaults for R are too big. See the previous notebooks for more details of the options settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options(repr.plot.width=4, repr.plot.height=4)  # Set plot height and width (in inches)\n",
    "\n",
    "hist(hw2$height)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R has good defaults for making quick graphics. Here it uses the data frame name and the column name for the title and the x-axis, and the word frequency (the same as \"count\") for the y-axis. This graph is not pretty, but it is functional.\n",
    "\n",
    "Part of the R philosophy is doing many analyses quickly while you're figuring out what your data means. So the expectation is that most of the figures you make will be looked at and then deleted or ignored. You should not spend a lot of time making the graphs look pretty, until you are sure they are what you want. R supports this behavior in particular. \n",
    "\n",
    "That said, R can make spectacular publication quality graphics. To allow this, there are many options that can be set. We will show a few here, but publication quality graphics is beyond the scope of this presentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we set a few more options, compare with the figure above\n",
    "# Note that the options set above persist until we manually change them!\n",
    "\n",
    "hist(hw2$height, col=\"light blue\", xlab = \"Height in Inches\", ylab = \"Count\", main =\"Height Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your turn: make a histogram of weights. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review: Get the mean and standard deviation of the weights\n",
    "#    The commands are in the previous notebook\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Probabilistic Models (Normal)\n",
    "\n",
    "The simplest probability models assume that your data:\n",
    "\n",
    "+ Come from a fixed probability distribution\n",
    "+ This distribution can be described by just a couple of numbers (called **parameters**)\n",
    "\n",
    "If we go on to assume that the shape is a bell-curve, then we have a **normal** or **Gaussian** probability model. (Different fields prefer one or the other of these names; you will also see a related name/concept called the \"error function.\") This bell-shaped curve comes up a lot. For instance, it can be mathematically proven to arise when you have either (i) a simple repeated measurement with error or (ii) when a measurement has many small factors that add together to determine it. This latter is part of the shape's ubiquity in biology.\n",
    "\n",
    "When you are in this situation, you usually \"fit\" the distribution to the data.\n",
    "\n",
    "**For the normal curve, you have fit it as soon as you have the mean and standard deviation of the data.** This is why we like the normal, it is very easy to fit. And you have been doing so since your stats 101 class, possibly without realizing it.\n",
    "\n",
    "So, given the results of the previous code cell, you have already \"fit\" the Gaussian distribution to the weight data! In the cell below we will look at how well the Gaussian model fits the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our original data (in red)\n",
    "\n",
    "hist(hw2$weight, col=rgb(1,0,0,0.5), main = \"Weight Data\", xlab = \"Weight\", probability = TRUE)\n",
    "\n",
    "# Simulated normal data (in blue)\n",
    "# Note that this command is spread over TWO lines as it is very long\n",
    "\n",
    "hist(rnorm(1000, mean = mean(hw2$weight), sd = sd(hw2$weight)), col=rgb(0,0,1,0.5), \n",
    "     probability=TRUE, add=TRUE) \n",
    "\n",
    "# We discuss the details of this BELOW the figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's analyze the figure we just made.\n",
    "\n",
    "What we have done here is make a histogram of our weight sample (in red) and we have added an overlay of 1000 random normal numbers from a normal distribution where the mean of the distribution is set equal to the mean of the sample and the standard deviation of the distribution is set to the standard deviation of the sample. The normal distribution is flexible because these two aspects of it,  its center location and its width (or spread), can be set to any value we like.\n",
    "\n",
    "In essence, we told R:\n",
    "\n",
    "+ Make a normal distribution with mean and standard deviation equal to the sample\n",
    "+ Now make up 1000 fictitious weights from this distribution\n",
    "+ Show us how the historgram of data (red) compares with the histogram of the fictitious data (blue)\n",
    "\n",
    "As you can see, the histograms are similar, but not identical. This is what is expected to happen when you build a probability model.\n",
    "\n",
    "**Details:**\n",
    "\n",
    "+ The first histogram should be recognizable from the work above -- it is just the data again, with titles. \n",
    "+ `col` is color, which is here set to a red-green-blue-alpha (RGBA) value using the `rgb` function: each of red, green, and blue run from 0 (none) to 1 (maximum amount); alpha is transparency. Setting alpha to 0.5 means that where they overlap the colors will mix equally. Using various levels of red, green, and blue, each between 0 and 1, you can make any color you want.\n",
    "+ Here we used _pure_ red (`rgb(1,0,0,0.5)` and _pure_ blue (`rgb(0,0,1,0.5)`). Both with alpha of 0.5. Where the two histograms overlap we get purple (red + blue = purple).\n",
    "+ `probability` is a Boolean value (`TRUE` or `FALSE`) that, when set to true, tells R to convert the counts in the histogram to density units (scaled counts). This makes histograms with different numbers of data points comparable. Compare this with the histogram you made above. (R likes to use `TRUE` and `FALSE` to turn features on and off, respectively.)\n",
    "+ `add` is an option, used in the second histogram that tells R to place this on top of the previous histogram rather than making a new one.\n",
    "+ Finally, this example shows that we can split functions and commands over multiple lines. In this case, we split the line after a comma. R knows that this cannot be a complete function, it looks for the final closing round bracket, so it keeps reading until it finds one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise: We know the weights have a mean of 127.22 pounds and a standard \n",
    "# deviation of 11.96 pounds. What if we tried to model them using a normal \n",
    "# distribution with a mean of 130 and sd of 12. Would you be able to tell?\n",
    "#\n",
    "# Recreate the figure above with the normal mean = 130, and sd = 12. Do the \n",
    "# histograms overlap?\n",
    "#\n",
    "# Hint: Don't forget to copy the command for both the real data and the fictitious\n",
    "#       histogram!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise: What if the normal model had a mean weight of 100 and sd of 12?\n",
    "# Hint: Your first try might look a little weird!\n",
    "# Hint: add the parameter \"xlim = c(60,160)\" (without the quotes) to the \n",
    "#       specifications of the data histogram.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key thing to note in the above examples is that as the theoretical normal curve (the fictituous data) is moved away from the real data, it looks less and less like a correct description of the data. Much of statistical work is finding a model, like the normal here, that adequately describes the data in your research.\n",
    "***\n",
    "### More Advanced Example (Optional)\n",
    "\n",
    "If the following example is beyond your statistical training it may be skipped for now.\n",
    "\n",
    "R has the capability to draw the pure (methematicaly defined) shape of many distributions. It is common in textbooks to overlay this shape onto histograms. We can do this here as well. It takes three steps:\n",
    "\n",
    "1. Make a list of x-axis values (here weights). Use **many** values, for this data we will use a step size of 0.5 lbs.\n",
    "1. For each of the x-axis values, get the theoretically defined normal value. These come from `dnorm` (the **density function** for the normal). This makes a second list of y-axis values.\n",
    "1. Use the `lines` function to add these (x,y) points to the figure. If you give `lines` two lists, it will treat the first as x-coordinates, and the second as y-coordinates.\n",
    "\n",
    "Here we go:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with the histogram again\n",
    "\n",
    "hist(hw2$weight, col=\"gray\", main = \"Weight Data\", xlab = \"Weight\", probability = TRUE)\n",
    "\n",
    "# The following lines make the normal bell-curve and add it to the plot\n",
    "\n",
    "x = seq(90, 170, 0.25)  # Make a list of weights that spans the x-axis\n",
    "\n",
    "y = dnorm(x, mean = mean(hw2$weight), sd = sd(hw2$weight))  # Get the normal height for each x\n",
    "\n",
    "lines(x, y, col=\"blue\", lwd = 2)  # Add the normal curve to the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in this example we expect the histogram too closely track the density, that is the smooth curve, but we also don't expect them to be identical. Do not demand from your data things which are impossible.\n",
    "\n",
    "We will not dicuss where the theoretical values come from, that is too advanced for this workshop, but these numbers are the values for the **normal density function** if you know what that means.\n",
    "***\n",
    "## 4.3 Bivariate Data Visualization & Elementary Summary Statistics\n",
    "\n",
    "As we have a bivariate data set, let's analyze it as such. Usually, we want to see the effect that each variable has on the other. This is easily achieved with a scatter plot, which you have actually already made for this data in the previous notebook. Let's remake that figure here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a scatterplot of weight (y) as a function of height (x)\n",
    "# You did this previously, with the plot function (try ?plot if you need to!)\n",
    "# For fun, if you have time, try to make the plot characters solid blue dots (Hint: ?pch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Straight Line Fits\n",
    "\n",
    "We usually summarize bivariate relationships with a **regression line**, a straight line that optimally passes through the cloud of points. To do this we need to **fit a linear model** to this data set. This brings us to the first use of R's **model language**. \n",
    "\n",
    "Now, this is an introduction to R, not an introduction to statistics, but we will try to summarize the basic ideas here. Hopefully you have seen this before in a class, if not, try to focus on the commands for now, and come back when you get some training in this topic.\n",
    "\n",
    "Linear regression is the simplest case of creating a functional relationship between two variables. We will work with `weight` as a function of `height`. That is \n",
    "\n",
    "$$\\mathrm{weight} = \\beta_0 + \\beta_1 \\mathrm{height}$$ \n",
    "\n",
    "which says \"weight is a linear (straight line) function of height.\" If you recall from high school math classes, a straight line is represented by a formula \n",
    "\n",
    "$$y = a + bx$$ \n",
    "\n",
    "where $x$ and $y$ are the variables, $a$ is the y-intercept, and $b$ is the _slope_ of the line. (This is also sometimes written $y = y_0 + mx$ or $y = mx + b$. While the variables change name, there is always a $y$ on the left, and $x$ on the right, and, also on the right, something that multiplies the $x$ and something that is added to it. We will use the form we first gave above today.)\n",
    "\n",
    "To fit a straight line to a data set, you have to determine the values of $a$ and $b$, the intercept and slope. This was one of the [earliest of the data reduction techniques discovered](https://en.wikipedia.org/wiki/Least_squares) that is still in widespread use. We will not discuss this in detail, it is beyond the scope of this workshop, but procedures for doing this are automatic, and implemented in R.\n",
    "\n",
    "Although you do not need to know a lot for today, here are some resources for future reference: \n",
    "+ For an excellent summary of the language for people who know about linear models already, see [Richard Hahn's Famous Handout](http://faculty.chicagobooth.edu/richard.hahn/teaching/formulanotation.pdf). \n",
    "+ For an intermediate review of linear regression, try [this link from r-statistics.co](http://r-statistics.co/Linear-Regression.html). \n",
    "+ Finally, for the total beginner, try [this link from tutorialspoint](https://www.tutorialspoint.com/r/r_linear_regression.htm).\n",
    "\n",
    "To tell R what straight line to fit, we need to tell it what the $x$ and $y$ are in the equation above. We do this by writing an R **formula**. In math language, we wrote this above:\n",
    "\n",
    "$$\\mathrm{weight} = \\beta_0 + \\beta_1 \\mathrm{height}$$ \n",
    "\n",
    "in R notation, this is written: `weight ~ height`. R will know to fill in everything else. The idea is that in R we write down a linear model in the same _basic_ form as in math, listing the variables we want to use, but we use the `~` to seperate the response variable from the predictor variable, rather than an equals sign. \n",
    "\n",
    "In statistics a linear regression is a type of **linear model** and in R we use the function `lm` to estimate these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here are the intercept and slope for this data\n",
    "\n",
    "lm(weight ~ height, data = hw2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This prints out our intercept and slope. Because `lm` can use multiple predictor (x) variables, instead of using the label slope, it labels each slope with the variable it is paired with in the function. \n",
    "\n",
    "**Notice how R uses the `data` option in the `lm` function.** Most of R's statistical modeling functions have this feature that allows you to write formulas without including the data frame name in the variables. Unfortunately, not every function has this feature! R is not always consistent in how functions are defined. When it is available, we will use this as it makes the models easier to discuss and fit.\n",
    "\n",
    "R allows us to do it the other way, using the full name and address of each variable, but when modeling we will usually prefer to use the `data` option for the `lm` and similar functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just to prove it works:\n",
    "\n",
    "lm(hw2$weight ~ hw2$height)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to use the information in a straight line fit for other purposes, we need to store the output in a variable. \n",
    "\n",
    "Here we just want to use the `lm` output to draw the best-fitting straight line for the scatterplot you made above. This is very easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is my version of the plot you made above:\n",
    "\n",
    "options(repr.plot.width=5, repr.plot.height=4)  # Wider for the long title!\n",
    "\n",
    "plot(hw2$height, hw2$weight, pch = 16, col = \"gray\", xlab = \"Height\", \n",
    "     ylab = \"Weight\", main = \"Relationship Between Height and Weight\")\n",
    "\n",
    "# Fit the linear model:\n",
    "hw.model <- lm(weight ~ height, data = hw2)\n",
    "\n",
    "# Draw the line:\n",
    "abline(hw.model, col = \"blue\", lwd = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Study the code in the cell above, it is the model for the next exercise.\n",
    "\n",
    "The `lm` function figures out the formula for the straight line, and the `abline` function adds the line to the scatterplot. The `abline` function is powerful, to learn more look up the help: `?abline`. It is also smart &mdash; it knows how to draw different lines based on what you give it. Here we give it the output of a linear model, and it knows to draw the model's regression line.\n",
    "\n",
    "Now it is your turn. Let's use the `mtcars` data set to make another linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data(mtcars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the first few rows of the mtcars data set \n",
    "# Hint: use head, or something inside the []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at miles per gallon (mpg; y-axis) as a function of displacement (disp; x-axis)\n",
    "# Make the scatterplot, remember that plot uses the order plot(x,y,...)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Displacement is the size of the engine. What is the relationship between miles per gallon and engine size? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the linear model with lm, note that with the tilde the order is y ~ x\n",
    "# Remember to store the results in a variable so you can use it for the line!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the code you made above for the scatterplot \n",
    "\n",
    "# Add the command for abline \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One minor point worth mentioning: when making plots in R with Jupyter, you need to have **all** of the plot commands inside of a single cell. Plot commands spread out across several cells will not work. If you use RStudio on your own computer, you will find that you can issue commands in sequence and when you use a function like `abline`, it will apply to the **most recent plot** made by R. Jupyter does not do this!\n",
    "\n",
    "### Correlations\n",
    "\n",
    "For data that is well described by a straight line, we can get an assessment of the strength of the relationship by using the bivariate correlation. This is also called the Pearson correlation. R has a number of functions for working with correlations, you already saw a brief glimpse of these in an earlier notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the correlation between displacement and miles per gallon\n",
    "# Hint: ?var helped you with this in a previous notebook\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspecting of the picture you made above should convince you that the relationship described here is likely to be a meaningful one, but if you want to formally test this, use `cor.test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the significance test of the correlation with cor.test\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is the correlation significant? (Let $\\alpha = 0.01$.)\n",
    "***\n",
    "## 4.4 Two Group analyses\n",
    "\n",
    "The simplest way to compare two groups of numerical measurements (in a formal way) is the [t-test](https://en.wikipedia.org/wiki/Student%27s_t-test). R implements a sophisticated collection of t-test methods in the function `t.test`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the help for the t.test function\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's re-create the state data used at the end of the last notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s <- data.frame(name = state.abb, region = state.region, area = state.area)\n",
    "head(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we want to do is compare the average state size in the different regions. Let's start with the boxplot as this shows us a good overview. \n",
    "\n",
    "For boxplots, when the boxes and/or whiskers do not overlap that indicates that the groups (here, regions) **may** have a real difference in the measurement (here, the areas of the states). Look closely at the next figure, where do the boxes overlap? Where do the lines overlap? Where are they closest?\n",
    "\n",
    "You will want to try different values for the `ylim` (y-axis limits) to \"zoom\" in on the figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options(scipen=999)  # You have seen all of this before\n",
    "\n",
    "boxplot(s$area ~ s$region, ylim = c(0, 175000)) # Change y-axis to zoom in on the figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously you used an expression like `s$area[s$region == \"South\"]` to pick out all of the areas in the \"South\" region of the US. To use the t-test function, you will need to pick out the areas from two regions, and give these to `t.test` to analyze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the areas of the states in the South\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the areas of the states in the North Central region\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the two expressions you used in the last two code blocks and give them to t.test\n",
    "# Order here does not matter\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are the South and North Central regions significantly different in state areas? Why or why not? To make things concrete, use an $\\alpha = 0.05$.\n",
    "\n",
    "Since the South and North Central regions have boxes very close together on the boxplot, we should not be surprised that there is not a significant difference in average state area. Pick two other regions that are further apart (in terms of the overlap of their boxes in the boxplot) and see what `t.test` reports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the t-tests here, keep picking regions until you get a significant result!\n",
    "# Note: This is for learning purposes, do not conduct real research by just trying\n",
    "#       everything until you get significance. That leads to the \"replication crisis\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Version 1.0  \n",
    "2018.06.06  \n",
    "\n",
    "To contact the author, please email [mturner46@gsu.edu](mailto:mturner46@gsu.edu). Please contact me with recommendations for improvement or if you find any errors. This work may be adapted for any non-commercial purpose within the bounds of the license.\n",
    "\n",
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png\" /></a><br />This work is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
